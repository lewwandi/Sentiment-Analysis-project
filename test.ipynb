{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1520dae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder,label_binarize,StandardScaler\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from itertools import cycle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold,learning_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2,SelectPercentile\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import gc\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e305c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_sample_data(sample_size=20000):\n",
    "    splits = {'train': 'train_df.csv', 'validation': 'val_df.csv', 'test': 'test_df.csv'}\n",
    "    dfs = {}\n",
    "    for split, file in splits.items():\n",
    "        df = pd.read_csv(\"hf://datasets/Sp1786/multiclass-sentiment-analysis-dataset/\" + file)\n",
    "        print(f\"Original {split} dataset size: {len(df)}\")\n",
    "        if len(df) > sample_size:\n",
    "            df = df.sample(n=sample_size, random_state=42)\n",
    "            print(f\"Sampled {split} to {len(df)} rows for faster training\")\n",
    "        dfs[split] = df\n",
    "    return dfs['train'], dfs['validation'], dfs['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d154ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train dataset size: 31232\n",
      "Sampled train to 20000 rows for faster training\n",
      "Original validation dataset size: 5205\n",
      "Original test dataset size: 5206\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df = load_and_sample_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03593264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Handle contractions better\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"'re\", \" are\", text)\n",
    "    text = re.sub(r\"'ve\", \" have\", text)\n",
    "    text = re.sub(r\"'ll\", \" will\", text)\n",
    "    text = re.sub(r\"'d\", \" would\", text)\n",
    "    text = re.sub(r\"'m\", \" am\", text)\n",
    "    \n",
    "    # Remove URLs, HTML tags, and email addresses\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Handle repeated characters (e.g., \"sooo good\" -> \"so good\")\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "    \n",
    "    # Remove numbers but keep emoticons\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # # Keep some punctuation that might be important for sentiment\n",
    "    text = re.sub(r'[^\\w\\s!?]', ' ', text)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a39c2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to D:/7th SEMESTER/Machine\n",
      "[nltk_data]     Learning/modelnew/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\", download_dir=\"D:/7th SEMESTER/Machine Learning/modelnew/nltk_data\")\n",
    "with open('nltk_data/corpora/stopwords/english', 'r') as file:\n",
    "    stopwords = set(file.read().splitlines())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e70ecc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curves(y_true, y_pred_proba, classes, model_name):\n",
    "    \"\"\"Plot ROC curves for multiclass classification\"\"\"\n",
    "    y_true_bin = label_binarize(y_true, classes=classes)\n",
    "    n_classes = y_true_bin.shape[1]\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_proba[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # Plot all ROC curves\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                 label=f'ROC curve of class {classes[i]} (area = {roc_auc[i]:0.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curves for {model_name}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "842a8ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes, model_name):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(f'Confusion Matrix for {model_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c16b781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(model, X, y, model_name, cv=5):\n",
    "    \"\"\"Plot learning curves for a given model\"\"\"\n",
    "    # Define training set sizes\n",
    "    train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "    \n",
    "    # Calculate learning curves\n",
    "    train_sizes_abs, train_scores, val_scores = learning_curve(\n",
    "        model, X, y, \n",
    "        train_sizes=train_sizes,\n",
    "        cv=cv,\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Calculate mean and standard deviation\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    val_mean = np.mean(val_scores, axis=1)\n",
    "    val_std = np.std(val_scores, axis=1)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot training scores\n",
    "    plt.plot(train_sizes_abs, train_mean, 'o-', color='blue', \n",
    "             label='Training Score', linewidth=2, markersize=6)\n",
    "    plt.fill_between(train_sizes_abs, train_mean - train_std, train_mean + train_std, \n",
    "                     alpha=0.1, color='blue')\n",
    "    \n",
    "    # Plot validation scores\n",
    "    plt.plot(train_sizes_abs, val_mean, 'o-', color='red', \n",
    "             label='Validation Score', linewidth=2, markersize=6)\n",
    "    plt.fill_between(train_sizes_abs, val_mean - val_std, val_mean + val_std, \n",
    "                     alpha=0.1, color='red')\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlabel('Training Set Size', fontsize=12)\n",
    "    plt.ylabel('F1 Score (Weighted)', fontsize=12)\n",
    "    plt.title(f'Learning Curve - {model_name}', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc='lower right', fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add performance indicators\n",
    "    final_train_score = train_mean[-1]\n",
    "    final_val_score = val_mean[-1]\n",
    "    gap = final_train_score - final_val_score\n",
    "    \n",
    "    \n",
    "    # Color-code the gap interpretation\n",
    "    if gap > 0.5:\n",
    "        gap_color = 'red'\n",
    "        gap_text = 'High Overfitting'\n",
    "    elif gap > 0.1:\n",
    "        gap_color = 'orange'\n",
    "        gap_text = 'Moderate Overfitting'\n",
    "    else:\n",
    "        gap_color = 'green'\n",
    "        gap_text = 'Good Generalization'\n",
    "    \n",
    "    plt.text(0.98, 0.02, f'Status: {gap_text}', transform=plt.gca().transAxes, \n",
    "             fontsize=11, ha='right', va='bottom',\n",
    "             bbox=dict(boxstyle='round', facecolor=gap_color, alpha=0.3))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return train_sizes_abs, train_mean, val_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "06d225e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_learning_curves(results, X, y, cv=5):\n",
    "    \"\"\"Plot learning curves for all trained models\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"GENERATING LEARNING CURVES FOR ALL MODELS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    learning_data = {}\n",
    "    \n",
    "    for model_name, result in results.items():\n",
    "        print(f\"\\nGenerating learning curve for {model_name}...\")\n",
    "        model = result['model']\n",
    "        \n",
    "        try:\n",
    "            train_sizes, train_scores, val_scores = plot_learning_curves(\n",
    "                model, X, y, model_name, cv\n",
    "            )\n",
    "            learning_data[model_name] = {\n",
    "                'train_sizes': train_sizes,\n",
    "                'train_scores': train_scores,\n",
    "                'val_scores': val_scores\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating learning curve for {model_name}: {str(e)}\")\n",
    "    \n",
    "    return learning_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a8ac4f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (20000,), Test set: (5206,), Validation set: (5205,)\n"
     ]
    }
   ],
   "source": [
    "x_train = train_df[\"text\"]\n",
    "y_train = train_df[\"sentiment\"]\n",
    "x_val = val_df[\"text\"]\n",
    "y_val = val_df[\"sentiment\"]\n",
    "x_test = test_df[\"text\"]\n",
    "y_test = test_df[\"sentiment\"]\n",
    "\n",
    "print(f\"Training set: {x_train.shape}, Test set: {x_test.shape}, Validation set: {x_val.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ad0ae02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorizer():\n",
    "    return TfidfVectorizer(\n",
    "        max_features=8000,        # Increased features\n",
    "        min_df=10,                  # Reduced min_df for more features\n",
    "        max_df=0.7,               # Slightly increased max_df\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 2),        # Include trigrams\n",
    "        sublinear_tf=True,         # Apply sublinear tf scaling\n",
    "        norm='l2',\n",
    "        use_idf=True,\n",
    "        smooth_idf=True,\n",
    "        lowercase=True,\n",
    "        token_pattern=r'[a-zA-Z]{3,}',\n",
    "        binary=False,              \n",
    "        strip_accents='unicode'\n",
    "    )\n",
    "\n",
    "tfidf_vec = create_vectorizer()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "86562803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_scores(y_true, y_pred, dataset_name=\"Dataset\"):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    print(f'\\n{dataset_name} Scores:')\n",
    "    print(f'  Accuracy: {acc:.4f}')\n",
    "    print(f'  Weighted F1: {f1_weighted:.4f}')\n",
    "    print(f'  Macro F1: {f1_macro:.4f}')\n",
    "    print(f'  Precision: {precision:.4f}')\n",
    "    print(f'  Recall: {recall:.4f}')\n",
    "    \n",
    "    return acc, f1_weighted, f1_macro\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ae670c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_pipeline():\n",
    "    return Pipeline([\n",
    "        ('feature_selection', SelectPercentile(chi2, percentile=80)),  # Keep top 80% of features\n",
    "        ('svd', TruncatedSVD(n_components=2000, random_state=42)),     # Dimensionality reduction\n",
    "        ('scaler', StandardScaler())                                   # Standardize features\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b42f7b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_probabilities(model, X, label_encoder):\n",
    "    \n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        probabilities = model.predict_proba(X)\n",
    "    elif hasattr(model, 'decision_function'):\n",
    "        decision_scores = model.decision_function(X)\n",
    "        \n",
    "        \n",
    "        if decision_scores.ndim == 1:\n",
    "            probabilities = np.column_stack([1 - decision_scores, decision_scores])\n",
    "        else:\n",
    "            exp_scores = np.exp(decision_scores - np.max(decision_scores, axis=1, keepdims=True))\n",
    "            probabilities = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    else:\n",
    "        # Fallback: use predict and create dummy probabilities\n",
    "        predictions = model.predict(X)\n",
    "        n_classes = len(label_encoder.classes_)\n",
    "        probabilities = np.zeros((len(predictions), n_classes))\n",
    "        for i, pred in enumerate(predictions):\n",
    "            probabilities[i, pred] = 1.0\n",
    "    \n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "27727e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multiple_models(x_train, x_val, x_test, y_train, y_val, y_test):\n",
    "\n",
    "    print(\"Applying text preprocessing...\")\n",
    "    x_train_processed = [text_preprocessing(text) for text in x_train]\n",
    "    x_val_processed = [text_preprocessing(text) for text in x_val]\n",
    "    x_test_processed = [text_preprocessing(text) for text in x_test]\n",
    "\n",
    "    # Vectorize processed text\n",
    "    tfidf_vec = create_vectorizer()\n",
    "    x_train_vectorized = tfidf_vec.fit_transform(x_train_processed)\n",
    "    x_val_vectorized = tfidf_vec.transform(x_val_processed)\n",
    "    x_test_vectorized = tfidf_vec.transform(x_test_processed)\n",
    "\n",
    "    # Initialize and fit LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "    y_val_encoded = label_encoder.transform(y_val)\n",
    "    y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "    print(\"Applying feature selection...\")\n",
    "    feature_pipeline = create_feature_pipeline()\n",
    "    x_train_processed_features = feature_pipeline.fit_transform(x_train_vectorized, y_train_encoded)\n",
    "    x_val_processed_features = feature_pipeline.transform(x_val_vectorized)\n",
    "    x_test_processed_features = feature_pipeline.transform(x_test_vectorized)\n",
    "\n",
    "    # validation\n",
    "    cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    models = {\n",
    "        'Regularized Logistic Regression': {\n",
    "            'model': LogisticRegression(\n",
    "                random_state=42, \n",
    "                max_iter=1000,\n",
    "                class_weight='balanced',\n",
    "                fit_intercept=True,\n",
    "                multi_class='multinomial'\n",
    "            ),\n",
    "            'params': {\n",
    "                'C': [0.0001, 0.001, 0.01],  # Stronger regularization\n",
    "                'penalty': ['l2'],   # Try both L1 and L2\n",
    "            }\n",
    "        },\n",
    "        'Regularized Linear SVM': {\n",
    "            'model': LinearSVC(\n",
    "                random_state=42, \n",
    "                max_iter=2000,\n",
    "                class_weight='balanced',\n",
    "                dual=False,\n",
    "            ),\n",
    "            'params': {\n",
    "                'C': [0.0001, 0.001, 0.01],  # Stronger regularization\n",
    "                'penalty': ['l2'],\n",
    "                'loss': ['hinge', 'squared_hinge']\n",
    "            }\n",
    "        },\n",
    "        'Regularized Random Forest': {\n",
    "            'model': RandomForestClassifier(\n",
    "                random_state=42, \n",
    "                class_weight='balanced',\n",
    "                n_jobs=-1,\n",
    "                bootstrap=True,\n",
    "                oob_score=True,    \n",
    "            ),\n",
    "            'params': {\n",
    "                'n_estimators': [100],\n",
    "                'max_depth': [5],      # Reduced depth\n",
    "                'min_samples_split': [50], # Increased\n",
    "                'min_samples_leaf': [25],  # Increased\n",
    "                'max_features': ['sqrt', 'log2'],  # More restrictive\n",
    "                'max_samples': [0.6],\n",
    "                'min_impurity_decrease': [0.001]      # Bootstrap sampling ratio\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    classes = label_encoder.classes_\n",
    "    \n",
    "    for name, config in models.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training {name}...\")\n",
    "\n",
    "        gc.collect()\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            config['model'],\n",
    "            config['params'],\n",
    "            cv=cv_strategy,\n",
    "            scoring='f1_weighted',\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "            return_train_score=True\n",
    "        )\n",
    "        \n",
    "        # Fit the model\n",
    "        grid_search.fit(x_train_processed_features, y_train_encoded)\n",
    "        \n",
    "        # Get best model\n",
    "        best_model = grid_search.best_estimator_\n",
    "        \n",
    "        # Make predictions\n",
    "        y_train_pred = best_model.predict(x_train_processed_features)\n",
    "        y_val_pred = best_model.predict(x_val_processed_features)\n",
    "        y_test_pred = best_model.predict(x_test_processed_features)\n",
    "        \n",
    "        y_train_pred_decoded = label_encoder.inverse_transform(y_train_pred)\n",
    "        y_val_pred_decoded = label_encoder.inverse_transform(y_val_pred)\n",
    "        y_test_pred_decoded = label_encoder.inverse_transform(y_test_pred)\n",
    "        \n",
    "        # Calculate scores\n",
    "        train_acc, train_f1_w, train_f1_m = detailed_scores(y_train, y_train_pred_decoded, \"Training\")\n",
    "        test_acc, test_f1_w, test_f1_m = detailed_scores(y_test, y_test_pred_decoded, \"Test\")\n",
    "        \n",
    "        \n",
    "        overfitting_score = train_acc - test_acc\n",
    "        print(f\"\\nOverfitting Score (Train - Test): {overfitting_score:.4f}\")\n",
    "        if overfitting_score > 0.05:\n",
    "            print(\" WARNING: Model shows signs of overfitting!\")\n",
    "       \n",
    "        results[name] = {\n",
    "            'model': best_model,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'cv_score': grid_search.best_score_,\n",
    "            'train_accuracy': train_acc,\n",
    "            'test_accuracy': test_acc,\n",
    "            'train_f1_weighted': train_f1_w,\n",
    "            'test_f1_weighted': test_f1_w,\n",
    "            'overfitting_score': overfitting_score,\n",
    "            'y_test_pred': y_test_pred_decoded\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nBest Parameters: {grid_search.best_params_}\")\n",
    "        print(f\"Cross-validation Score: {grid_search.best_score_:.4f}\")\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plot_confusion_matrix(y_test, y_test_pred_decoded, classes, name)\n",
    "        \n",
    "        # Get probability predictions for ROC curves\n",
    "        try:\n",
    "            print(\"Generating ROC curves...\")\n",
    "            y_test_proba = get_model_probabilities(best_model, x_test_processed_features, label_encoder)\n",
    "            plot_roc_curves(y_test_encoded, y_test_proba, np.arange(len(classes)), name)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not generate ROC curves for {name}: {str(e)}\")\n",
    "    \n",
    "    return results, label_encoder, tfidf_vec, feature_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "86e2ef23",
   "metadata": {},
   
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Preparing data for learning curves...\")\n",
    "x_train_processed = [text_preprocessing(text) for text in x_train]\n",
    "x_train_vectorized = tfidf_vec.transform(x_train_processed)\n",
    "y_train_encoded = label_encoder.transform(y_train)\n",
    "x_train_processed_features = feature_pipeline_fitted.transform(x_train_vectorized)\n",
    "\n",
    "learning_data = plot_all_learning_curves(results, x_train_processed_features, y_train_encoded, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7504d279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logistic_regression': {'model': LogisticRegression()}}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9736972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, vectorizer, model, label_encoder):\n",
    "    processed_text = text_preprocessing(text)\n",
    "    vectorized_text = vectorizer.transform([processed_text])\n",
    "    processed_features = create_feature_pipeline.transform(vectorized_text)\n",
    "    prediction_encoded = model.predict(processed_features)[0]  \n",
    "    prediction_decoded = label_encoder.inverse_transform([prediction_encoded])[0]  \n",
    "    return prediction_decoded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
